{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Set Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook assumes you have done the setup required in Week 1 and 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outline\n",
    "\n",
    "- [Train-Validation-Test Split](#section1)\n",
    "- [Pipeline](#section2)\n",
    "- [Hypertuning](#section3)\n",
    "    - [Grid Search](#section3a)\n",
    "    - [Priotiritizing Parameters](#section3b)\n",
    "    - [Other Strategies](#section3c)\n",
    "- [Troubleshooting](#section4)\n",
    "    - [Imbalanced Classes](#section4a)\n",
    "    - [Information Leakage](#section4b)\n",
    "- [Lab](#section5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'section1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train-Validation-Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you begin fitting a model on a new dataset you should, almost always, split your initial dataset into a \"train\" dataset, a \"validation\" dataset and a \"test\" dataset.  The train dataset gives us a way to have our model \"learn\".  The validation dataset gives us a way to judge the performance of the model against other potential models.  The test dataset gives us an idea of how well our model generalizes for **unseen** data.  \n",
    "\n",
    "In practice, we will use the train dataset to train all your potential models.  The validation dataset will be passed to each of these models to judge the performance of each of these models allowing us to compare models against eachother.  Then once finding our optimal model we finally pass the test dataset to judge the model's performance on **unseen** data and the performance based on the test dataset will be the one reported in your academic paper or to your employer.\n",
    "\n",
    "You should generally keep 20-50% of the data for the validation and test sets and use the remaining 50-80% for training.\n",
    "\n",
    "Never just split your data into the first 80% and the remaining 20% for your validation and test sets.  You should always split your data as randomly as possible. The slightest inclusion of a non-random process in the selection of the training set can skew model parameters. Data is frequently sorted in some way (by date or even by the value you are trying to predict).\n",
    "\n",
    "There is a method implemented in Scikit that splits the dataset randomly for us called [train_test_split](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html#sklearn.model_selection.train_test_split).  We can use this method twice to perform a train-validation-test split done below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![a](images/dataset.png)\n",
    "![a](images/testtrainvalidation.png)\n",
    "[source](https://cdn-images-1.medium.com/max/948/1*4G__SV580CxFj78o9yUXuQ.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# make the split reproducible\n",
    "random_state = 42\n",
    "\n",
    "# get some data\n",
    "digits = load_digits()\n",
    "X, y = digits.data, digits.target\n",
    "\n",
    "# get our split percentages\n",
    "validation_size = .25\n",
    "test_size = .25\n",
    "validation_test_size = validation_size + test_size\n",
    "test_size_adjusted = test_size / validation_test_size\n",
    "\n",
    "# perform the first split which gets us the train data and the validation/test data that\n",
    "# we must split one more time\n",
    "X_train, X_validation_test, y_train, y_validation_test =  train_test_split(X, y,\\\n",
    "                                                                           test_size = validation_test_size,\\\n",
    "                                                                           random_state = random_state)\n",
    "\n",
    "# perform the second split which splits the validation/test data into two distinct datasets\n",
    "X_validation, X_test, y_validation, y_test = train_test_split(X_validation_test, y_validation_test,\\\n",
    "                                                              test_size = test_size_adjusted,\\\n",
    "                                                              random_state = random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'section2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In data science a ***pipeline*** is a chain of modelling related tasks.  There can be up to $n$ modelling tasks in any given pipeline.  We start with some initial input, which is fed into the first modelling task.  The output of the first modelling task is then fed to the next second modelling task, and so on and so forth, until we reach the final modelling task and output.  \n",
    "\n",
    "In the context of this course, we use a pipeline with two modelling tasks.  The initial input is an article that we want to classify as fake news or not.  The first modelling task takes our article and embeds it.  The output of the first model, the embeddings, are fed into the final modelling task, the classifier.  The final output of our pipeline, the classification will indicate whether the initial input, the article, is fake or not.\n",
    "\n",
    "When using Scikit you can use its builtin pipelining feature to build pipelines using your Scikit models. To see how to use this tool you may look at this [example](http://scikit-learn.org/stable/auto_examples/model_selection/grid_search_text_feature_extraction.html#example-model-selection-grid-search-text-feature-extraction-py).  In this example, a text feature extractor is composed with a linear classifier that uses stochastic gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'section3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypertuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With any machine learning algorithm we must pass some set of parameters to initialize the model. For any model the set of hyperparameters we use depends on the data we are trying to train on.  The process of finding the optimal set of hyperparameters for your model for the given dataset is called ***hypertuning***.  The process of hypertuning innvolves training multiple models with different sets of hyperparameters and using some metric or culmination of metrics(i.e. F1 Score, Precision, Recall, etc.) to determine the optimal set of hyperparameters.  We choose the optimal set of hyperparameters based on the model using the optimal set of hyperparameters producing the best overall metrics for the validation and/or training set of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'section3a'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid searches are typically used when you don't know (and often don't care too much about the meaning of) a set of optimal parameters to a given estimator or set of estimators. They are essentially a set of for loops that try out a series of parameters and construct a single model for each case (hence a grid). Scikits has a [grid search class](http://scikit-learn.org/stable/modules/grid_search.html#grid-search) that will automate an exhaustive or optimized search for one or more estimator parameters.\n",
    "\n",
    "Also somewhat confusingly, people will often conflate \"pipeline\" and \"grid search\", sometimes using the former to mean the latter. You can do a grid search as part of a pipeline, using a final function to estimate model quality, and the output of the tested models as input. Scikits has an [example of this here](http://scikit-learn.org/stable/modules/pipeline.html#pipeline).\n",
    "\n",
    "There are two kinds of Grid Search, exhaustive and random."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![a](images/gridsearch.png)\n",
    "\n",
    "[source](https://cdn-images-1.medium.com/max/1920/1*Uxo81NjcpqNXYJCeqnK1Pw.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exhaustive\n",
    "\n",
    "Exhaustive grid search is nothing more than a series of for loops, each iterating over a dictionary of possible hyperparameter values. The best performance of any of the searched parameters is kept and the chosen hyperparameters are returned.  Scikit has a method for this, though you could write your own doing something similar to this pseudo-code:\n",
    "\n",
    "```\n",
    "results = {}\n",
    "parameter_vals = {'p1':[a_1,a_2...a_K], 'p2':[b_1, b_2, ... b_M], ... , 'pN':[zz_1, zz_2, ..., zz_N]}\n",
    "\n",
    "parameter_sets = generate_parameter_grid by exhaustive combinations\n",
    "for set in parameter_sets\n",
    "    test accuracy of model(set)\n",
    "results[set] = accuracy   \n",
    "return argmax(results)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A random search for parameter values uses a generating function (typically a selected distribution, i.e. rbf/beta/gamma with user-input parameters) to produce candidate value sets for the hyperparameters. This has one main benefits over an exhaustive search:\n",
    "\n",
    "    - A budget can be chosen independent of the number of parameters and possible values. Thus the user only has one parameter to handle.\n",
    "\n",
    "Below is an example of how to perform both a **random** and **exhaustive** gridsearch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomizedSearchCV took 3.41 seconds for 20 candidates parameter settings.\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.923 (std: 0.017)\n",
      "Parameters: {'bootstrap': False, 'criterion': 'gini', 'max_depth': None, 'max_features': 7, 'min_samples_leaf': 4, 'min_samples_split': 2}\n",
      "\n",
      "Model with rank: 2\n",
      "Mean validation score: 0.905 (std: 0.013)\n",
      "Parameters: {'bootstrap': True, 'criterion': 'gini', 'max_depth': None, 'max_features': 7, 'min_samples_leaf': 4, 'min_samples_split': 10}\n",
      "\n",
      "Model with rank: 3\n",
      "Mean validation score: 0.768 (std: 0.024)\n",
      "Parameters: {'bootstrap': True, 'criterion': 'gini', 'max_depth': 3, 'max_features': 1, 'min_samples_leaf': 2, 'min_samples_split': 2}\n",
      "\n",
      "GridSearchCV took 44.23 seconds for 22 candidate parameter settings.\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.761 (std: 0.026)\n",
      "Parameters: {'bootstrap': True, 'criterion': 'gini', 'max_depth': 3, 'max_features': 1, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "\n",
      "Model with rank: 2\n",
      "Mean validation score: 0.785 (std: 0.020)\n",
      "Parameters: {'bootstrap': True, 'criterion': 'gini', 'max_depth': 3, 'max_features': 1, 'min_samples_leaf': 1, 'min_samples_split': 3}\n",
      "\n",
      "Model with rank: 3\n",
      "Mean validation score: 0.761 (std: 0.030)\n",
      "Parameters: {'bootstrap': True, 'criterion': 'gini', 'max_depth': 3, 'max_features': 1, 'min_samples_leaf': 1, 'min_samples_split': 10}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from time import time\n",
    "from operator import itemgetter\n",
    "from scipy.stats import randint as sp_randint\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# load digit dataset\n",
    "digits = load_digits()\n",
    "# split data into inputs and output\n",
    "X, y = digits.data, digits.target\n",
    "\n",
    "# build a random forest classifier\n",
    "clf = RandomForestClassifier(n_estimators=20)\n",
    "\n",
    "\n",
    "# Utility function to report best scores\n",
    "def report(grid_scores, n_top=3):\n",
    "    # sort scores based on metric so we can grab the n_top models\n",
    "    top_scores = sorted(grid_scores, key=itemgetter(1), reverse=True)[:n_top]\n",
    "    # iterate over the n_top models\n",
    "    for i in range(n_top):\n",
    "        print(\"Model with rank: {0}\".format(i + 1))\n",
    "        print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
    "              grid_scores['mean_test_score'][i],\n",
    "              grid_scores['std_test_score'][i]))\n",
    "        print(\"Parameters: {0}\".format(grid_scores['params'][i]))\n",
    "        print(\"\")\n",
    "\n",
    "\n",
    "# specify parameters and distributions to sample from - \n",
    "# what methods might we consider that would improve these estimates?\n",
    "param_dist = {\"max_depth\": [3, None],\n",
    "              \"max_features\": sp_randint(1, 11),\n",
    "              \"min_samples_split\": sp_randint(2, 11),\n",
    "              \"min_samples_leaf\": sp_randint(1, 11),\n",
    "              \"bootstrap\": [True, False],\n",
    "              \"criterion\": [\"gini\", \"entropy\"]}\n",
    "\n",
    "# number of models we are going to train\n",
    "n_iter_search = 20\n",
    "# create our randomized gridsearch classifier\n",
    "#      clf, is the model we are performing the search on\n",
    "#      param_dist, is a dictionary of paramater distributions that we will sample over\n",
    "#      n_iter_search, number of models we are going to train\n",
    "#      True, the scores from our training for each model will be returned when we perform the gridsearch\n",
    "random_search = RandomizedSearchCV(clf, param_distributions=param_dist,\n",
    "                                   n_iter=n_iter_search, return_train_score=True)\n",
    "# start a timer so we know how long the random gridsearch took\n",
    "start = time()\n",
    "# perform the random gridsearch\n",
    "random_search.fit(X, y)\n",
    "print(\"RandomizedSearchCV took %.2f seconds for %d candidates\"\n",
    "      \" parameter settings.\" % ((time() - start), n_iter_search))\n",
    "# print the top 3 model outputs from the random gridsearch\n",
    "report(random_search.cv_results_)\n",
    "\n",
    "# use a full grid over all parameters. \n",
    "# The grid search will generate parameter sets for each and every one of these\n",
    "param_grid = {\"max_depth\": [3, None],\n",
    "              \"max_features\": [1, 3, 10],\n",
    "              \"min_samples_split\": [2,3,10],\n",
    "              \"min_samples_leaf\": [1, 3, 10],\n",
    "              \"bootstrap\": [True, False],\n",
    "              \"criterion\": [\"gini\", \"entropy\"]}\n",
    "\n",
    "# create an exhaustive gridsearch object\n",
    "#      clf, is the model we are performing the search on\n",
    "#      param_grid dictionary with the parameter settings the search will try\n",
    "#      True, the scores from our training for each model will be returned when we perform the gridsearch\n",
    "grid_search = GridSearchCV(clf, param_grid=param_grid, return_train_score=True)\n",
    "# start a timer so we know how long the exhaustive gridsearch took\n",
    "start = time()\n",
    "# perform the exhaustive gridsearch\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "print(\"GridSearchCV took %.2f seconds for %d candidate parameter settings.\"\n",
    "      % (time() - start, len(grid_search.cv_results_)))\n",
    "# print the top 3 model outputs from the exhaustive gridsearch\n",
    "report(grid_search.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'section3b'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Priotiritizing Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When hypertuning, it is critical to remember that not all hyperparameters have equal importance.  With most models, a subset of hyperparameters will have a major impact on the model's performance, while the remaining hyperparameters will do little to nothing to impact a model's performance or there is an established value that you should use for a hyperparameter regardless of the data.  Hence, our hypertuning should focus on finding optimal values for this subset of important hyperparameters. \n",
    "\n",
    "For example, with neural networks, the two most important hyperparameters to tune are the learning rate and weight regularization of the optimizer.  Both of these parameters control the rate at which the neural network learns.  If you are \"aggressive\" with these parameters then we might overshoot the optimal weights, though if we are too \"lenient\" with these parameters we might undershoot the optimal weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'section3c'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Strategies "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are other ways to perform hypertuning beside grid search.  One alternative is ***Bayesian Optimization***.  Bayesian Optimization approximates a posterior distribution based on the model you are trying to train to find the optimal set of hyperparameters.  Here is an [implementation in Python](https://github.com/fmfn/BayesianOptimization)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'section4'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Troubleshooting "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In data science there are a multitude of problems that can arise either on the data and/or modeling side.  Fortunately, for us, a lot of problems we face in data science have been encountered by others and approaches have been established to solves these problems.  In this section we will look at two common problems that arise in data science and some tools of the trade for how to address them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'section4a'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imbalanced Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Imbalanced Classes*** occurs when performing a classification learning and the a subset of the potential classes we could output make up a substantial majority of our data.  \n",
    "\n",
    "To ensure our model is able to learn about all classes we could use a model that is robust to class imbalances.  One example of a model that is robust to class imbalances is the class weighted Support Vector Machine.  Essentially this model places a higher penalty on misclassifying observations of the minority class causing the model to put equal importance to each class despite the disparity in number of observations for each class.  Scikit's version of this SVM can be found [here](http://scikit-learn.org/stable/auto_examples/svm/plot_separating_hyperplane_unbalanced.html#).\n",
    "\n",
    "Though sometimes we might want to use a model not robust to imbalanced classes for our imbalanced data(i.e. this course using XGBoost on our imbalanced article dataset).  In these cases it is best to resample your data such that your sampled data corrects the imbalance.  This balanced sampled data is then used to train your model without it being affected by the imbalanced data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![a](images/unbalanced.png)\n",
    "\n",
    "[source](http://www.svds.com/wp-content/uploads/2016/08/messy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'section4b'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information Leakage "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Information Leakage*** occurs when training data has extra information that makes it seem like our model produces better results than it actuall would in the \"real world\".  The usual way we combat this is by performing a train, validation and test split to our data.  We only use the test set to judge the how well our final model will perform when put into production.  Though, sometimes we do not have a sufficient amount of data to to have a pure test set.  One way to combat information leakage(i.e. insufficient data) is to perform a KFold Cross Validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KFold Cross Validation\n",
    "\n",
    "Ideally, when training a model, we'd like to   When we lack sufficient data, when can still gauge the performance of a model using KFold Cross Validation.  We can get a more accurate value of the error by using KFold Cross Validation.\n",
    "\n",
    "Basically, we break the data into k groups. We take one of these groups and make it the test set. We then train the data on the remaining groups and calculate the error on the test group. We can repeat this process k times and average all the results. This gives us a more accurate picture of the error.\n",
    "\n",
    "You can perform a KFold Cross Validation in Scikit using this [method](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html#sklearn.model_selection.KFold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![cross validation](images/cross-validation.jpg)\n",
    "\n",
    "([source](http://cse3521.artifice.cc/classification-evaluation.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'section5'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now you will create your own pipeline where you take the embeddings we created in Lecture 2 and feed them into XGBoost, that we learned about in Lecture 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Setup a pipeline where embeddings we created in Lecture 2 are fed into XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) How did your first iteration of the pipeline do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) How could we improve the performance of the pipeline?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) What parameters are important to tune for the [embedding process?](https://radimrehurek.com/gensim/models/doc2vec.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) What parameters are important to tune for [XGBoost?](http://xgboost.readthedocs.io/en/latest/python/python_api.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) Now that you know what parameters are important to both processes in the pipeline, hypertune both models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7) Are there any sources of information leakage? Explain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8) Is the data balanced? How do we know the balances of the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9) If the data is imbalanced what can we do to make our pipeline robust to the imbalances?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10) Should our test set be balanced or not?  Explain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11) Based on the data we have, should we perform KFold Cross Validation and/or a train-validation-test split?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12) If time permits, write some code so that we can have balanced classes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
